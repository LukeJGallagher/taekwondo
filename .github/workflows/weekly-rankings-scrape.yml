# Weekly World Taekwondo Rankings Scraper
# =========================================
# Runs Playwright scraper weekly to collect all 16 weight categories
# Uploads to Azure Blob Storage and commits summary to repo

name: Weekly Rankings Scraper (Playwright)

on:
  # Run every Monday at 06:00 UTC (before workday)
  schedule:
    - cron: '0 6 * * 1'

  # Allow manual trigger from GitHub UI
  workflow_dispatch:
    inputs:
      categories:
        description: 'Specific categories (e.g., M-58kg,F-67kg) or leave empty for all'
        required: false
        default: ''
      upload_to_azure:
        description: 'Upload to Azure Blob Storage'
        required: false
        default: true
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  DATA_DIR: 'data'

jobs:
  scrape-rankings:
    name: Scrape World Taekwondo Rankings
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas selenium webdriver-manager
          # Install Chrome for Selenium
          sudo apt-get update
          sudo apt-get install -y chromium-browser chromium-chromedriver

      - name: Run Selenium Rankings Scraper
        id: scrape
        run: |
          echo "Starting Selenium scraper at $(date)"

          # Build command with optional categories
          CMD="python scrape_rankings_by_category.py --output ${{ env.DATA_DIR }}"

          if [ -n "${{ github.event.inputs.categories }}" ]; then
            CMD="$CMD --categories ${{ github.event.inputs.categories }}"
          fi

          # Run scraper
          $CMD

          # Check if data was collected
          if [ -f "${{ env.DATA_DIR }}/rankings/world_rankings_latest.csv" ]; then
            ATHLETE_COUNT=$(wc -l < "${{ env.DATA_DIR }}/rankings/world_rankings_latest.csv")
            echo "athletes_collected=$((ATHLETE_COUNT - 1))" >> $GITHUB_OUTPUT
            echo "scrape_status=success" >> $GITHUB_OUTPUT
          else
            echo "athletes_collected=0" >> $GITHUB_OUTPUT
            echo "scrape_status=failed" >> $GITHUB_OUTPUT
          fi

          echo "Scraping completed at $(date)"

      - name: Upload to Azure Blob Storage
        if: ${{ steps.scrape.outputs.scrape_status == 'success' && (github.event.inputs.upload_to_azure == 'true' || github.event_name == 'schedule') }}
        env:
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
          AZURE_CONTAINER_NAME: ${{ secrets.AZURE_CONTAINER_NAME || 'taekwondo-data' }}
        run: |
          pip install azure-storage-blob

          python << 'EOF'
          import os
          from pathlib import Path
          from datetime import datetime
          from azure.storage.blob import BlobServiceClient

          connection_string = os.environ.get('AZURE_STORAGE_CONNECTION_STRING')
          container_name = os.environ.get('AZURE_CONTAINER_NAME', 'taekwondo-data')

          if not connection_string:
              print("No Azure connection string - skipping upload")
              exit(0)

          blob_service = BlobServiceClient.from_connection_string(connection_string)
          container = blob_service.get_container_client(container_name)

          # Upload rankings files
          data_dir = Path('data')
          timestamp = datetime.now().strftime('%Y%m%d')

          for csv_file in data_dir.glob('**/*.csv'):
              # Create blob path: rankings/world_rankings_20250121.csv
              blob_name = f"{csv_file.parent.name}/{csv_file.stem}_{timestamp}.csv"

              with open(csv_file, 'rb') as data:
                  container.upload_blob(name=blob_name, data=data, overwrite=True)
                  print(f"Uploaded: {blob_name}")

          # Also upload latest version
          latest_csv = data_dir / 'rankings' / 'world_rankings_latest.csv'
          if latest_csv.exists():
              with open(latest_csv, 'rb') as data:
                  container.upload_blob(name='rankings/world_rankings_latest.csv', data=data, overwrite=True)
                  print("Uploaded: rankings/world_rankings_latest.csv (latest)")

          print("Azure upload complete")
          EOF

      - name: Commit rankings summary to repo
        if: steps.scrape.outputs.scrape_status == 'success'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

          # Create/update summary file (not full data to avoid bloating repo)
          python << 'EOF'
          import pandas as pd
          import json
          from datetime import datetime
          from pathlib import Path

          data_dir = Path('data')
          summary = {
              'last_updated': datetime.now().isoformat(),
              'categories': {},
              'total_athletes': 0,
              'saudi_athletes': []
          }

          # Read latest rankings
          rankings_file = data_dir / 'rankings' / 'world_rankings_latest.csv'
          if rankings_file.exists():
              df = pd.read_csv(rankings_file)
              summary['total_athletes'] = len(df)

              # Count by category
              if 'weight_category' in df.columns:
                  summary['categories'] = df.groupby('weight_category').size().to_dict()

              # Extract Saudi athletes
              country_col = 'country' if 'country' in df.columns else 'MEMBER NATION'
              if country_col in df.columns:
                  saudi_df = df[df[country_col].str.upper().str.contains('KSA|SAUDI', na=False)]
                  for _, row in saudi_df.iterrows():
                      name_col = 'athlete_name' if 'athlete_name' in df.columns else 'NAME'
                      summary['saudi_athletes'].append({
                          'name': row.get(name_col, 'Unknown'),
                          'rank': int(row.get('rank', row.get('RANK', 0))),
                          'category': row.get('weight_category', 'Unknown')
                      })

          # Write summary
          summary_file = data_dir / 'rankings_summary.json'
          with open(summary_file, 'w') as f:
              json.dump(summary, f, indent=2)

          print(f"Summary: {summary['total_athletes']} athletes, {len(summary['saudi_athletes'])} Saudi")
          EOF

          # Commit if there are changes
          git add data/rankings_summary.json
          git diff --staged --quiet || git commit -m "Update rankings summary $(date +%Y-%m-%d)

          Athletes: ${{ steps.scrape.outputs.athletes_collected }}

          Automated by GitHub Actions"

          git push || echo "Nothing to push"

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rankings-data-${{ github.run_number }}
          path: |
            data/rankings/*.csv
            data/rankings/*.json
            data/athletes/*.csv
          retention-days: 30

      - name: Job Summary
        run: |
          echo "## Rankings Scrape Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ steps.scrape.outputs.scrape_status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Athletes Collected**: ${{ steps.scrape.outputs.athletes_collected }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Run Time**: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "data/rankings_summary.json" ]; then
            echo "### Saudi Athletes" >> $GITHUB_STEP_SUMMARY
            python -c "import json; d=json.load(open('data/rankings_summary.json')); [print(f\"- {a['name']}: Rank #{a['rank']} ({a['category']})\") for a in d.get('saudi_athletes', [])]" >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "No Saudi athletes found" >> $GITHUB_STEP_SUMMARY
          fi

  notify-on-failure:
    name: Notify on Failure
    needs: scrape-rankings
    if: failure()
    runs-on: ubuntu-latest
    steps:
      - name: Send failure notification
        run: |
          echo "Rankings scrape failed - check workflow logs"
          # Add Slack/email notification here if secrets are configured
