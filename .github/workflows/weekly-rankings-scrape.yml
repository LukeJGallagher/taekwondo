# Weekly World Taekwondo Rankings Scraper
# =========================================
# Scrapes ALL 16 weight categories and uploads to Azure Blob Storage

name: Weekly Rankings Scraper

on:
  # Run every Monday at 06:00 UTC
  schedule:
    - cron: '0 6 * * 1'

  # Allow manual trigger
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'

jobs:
  scrape-rankings:
    name: Scrape All Weight Categories
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas selenium webdriver-manager requests beautifulsoup4

      - name: Create data directories
        run: |
          mkdir -p data/rankings
          mkdir -p data/athletes

      - name: Run Rankings Scraper - All Categories
        id: scrape
        continue-on-error: true
        run: |
          python << 'EOF'
          import time
          import pandas as pd
          from pathlib import Path
          from datetime import datetime
          from selenium import webdriver
          from selenium.webdriver.common.by import By
          from selenium.webdriver.chrome.options import Options
          from selenium.webdriver.chrome.service import Service
          from selenium.webdriver.support.ui import WebDriverWait
          from selenium.webdriver.support import expected_conditions as EC
          from webdriver_manager.chrome import ChromeDriverManager
          from io import StringIO

          print("="*60)
          print("WORLD TAEKWONDO RANKINGS SCRAPER - ALL CATEGORIES")
          print("="*60)

          # All 16 Olympic weight categories
          WEIGHT_CATEGORIES = {
              # Men's categories
              'M-54kg': ('M', 'M_54'),
              'M-58kg': ('M', 'M_58'),
              'M-63kg': ('M', 'M_63'),
              'M-68kg': ('M', 'M_68'),
              'M-74kg': ('M', 'M_74'),
              'M-80kg': ('M', 'M_80'),
              'M-87kg': ('M', 'M_87'),
              'M+87kg': ('M', 'M_87P'),
              # Women's categories
              'F-46kg': ('F', 'F_46'),
              'F-49kg': ('F', 'F_49'),
              'F-53kg': ('F', 'F_53'),
              'F-57kg': ('F', 'F_57'),
              'F-62kg': ('F', 'F_62'),
              'F-67kg': ('F', 'F_67'),
              'F-73kg': ('F', 'F_73'),
              'F+73kg': ('F', 'F_73P'),
          }

          # Setup Chrome
          chrome_options = Options()
          chrome_options.add_argument('--headless=new')
          chrome_options.add_argument('--no-sandbox')
          chrome_options.add_argument('--disable-dev-shm-usage')
          chrome_options.add_argument('--disable-gpu')
          chrome_options.add_argument('--window-size=1920,1080')
          chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

          try:
              service = Service(ChromeDriverManager().install())
              driver = webdriver.Chrome(service=service, options=chrome_options)
              driver.set_page_load_timeout(30)
              print("[OK] Chrome initialized")
          except Exception as e:
              print(f"[ERROR] Chrome setup failed: {e}")
              exit(1)

          all_athletes = []
          categories_scraped = 0
          timestamp = datetime.now().strftime('%Y%m%d')
          output_dir = Path('data/rankings')
          output_dir.mkdir(parents=True, exist_ok=True)

          # SimplyCompete base URL for rankings
          base_url = "https://worldtaekwondo.simplycompete.com/widget/rankings?country=&lang=EN&cat="

          try:
              for category_name, (gender, category_code) in WEIGHT_CATEGORIES.items():
                  print(f"\n{'='*50}")
                  print(f"[SCRAPING] {category_name} ({gender})")
                  print(f"{'='*50}")

                  try:
                      # Direct URL to SimplyCompete rankings for this category
                      url = f"{base_url}{category_code}"
                      print(f"[URL] {url}")

                      driver.get(url)
                      time.sleep(3)

                      # Wait for table to load
                      try:
                          WebDriverWait(driver, 15).until(
                              EC.presence_of_element_located((By.TAG_NAME, 'table'))
                          )
                      except:
                          print(f"[WARN] Timeout waiting for table in {category_name}")
                          continue

                      # Find tables
                      tables = driver.find_elements(By.TAG_NAME, 'table')

                      if not tables:
                          print(f"[WARN] No tables found for {category_name}")
                          continue

                      # Find the best table (most rows)
                      best_table = None
                      max_rows = 0
                      for table in tables:
                          rows = table.find_elements(By.TAG_NAME, 'tr')
                          if len(rows) > max_rows:
                              max_rows = len(rows)
                              best_table = table

                      if not best_table or max_rows < 2:
                          print(f"[WARN] No valid table found for {category_name}")
                          continue

                      # Parse with pandas
                      html = best_table.get_attribute('outerHTML')
                      dfs = pd.read_html(StringIO(html))

                      if dfs and len(dfs[0]) > 0:
                          df = dfs[0]

                          # Standardize columns
                          if len(df.columns) >= 5:
                              df.columns = ['rank', 'rank_change', 'athlete_name', 'country', 'points'] + list(df.columns[5:])

                          # Keep only needed columns
                          df = df[['rank', 'rank_change', 'athlete_name', 'country', 'points']].copy()

                          # Add metadata
                          df['gender'] = gender
                          df['weight_category'] = category_name
                          df['scraped_at'] = datetime.now().isoformat()

                          # Save individual category file
                          cat_file = output_dir / f'{category_name.replace("+", "P")}_rankings_{timestamp}.csv'
                          df.to_csv(cat_file, index=False)

                          all_athletes.append(df)
                          categories_scraped += 1

                          print(f"[OK] {category_name}: {len(df)} athletes")

                          # Check for Saudi athletes
                          saudi = df[df['country'].str.contains('KSA|Saudi', case=False, na=False)]
                          if len(saudi) > 0:
                              print(f"[KSA] Found {len(saudi)} Saudi athletes in {category_name}!")
                              for _, row in saudi.iterrows():
                                  print(f"  - {row['athlete_name']}: #{row['rank']}")
                      else:
                          print(f"[WARN] Empty table for {category_name}")

                  except Exception as e:
                      print(f"[ERROR] Failed to scrape {category_name}: {e}")
                      continue

                  # Brief pause between categories
                  time.sleep(2)

          except Exception as e:
              print(f"[ERROR] Scraping failed: {e}")
              import traceback
              traceback.print_exc()

          finally:
              driver.quit()
              print("\n[OK] Browser closed")

          # Combine all data
          if all_athletes:
              combined_df = pd.concat(all_athletes, ignore_index=True)

              # Save combined file
              combined_df.to_csv(output_dir / f'world_rankings_all_{timestamp}.csv', index=False)
              combined_df.to_csv(output_dir / 'world_rankings_latest.csv', index=False)

              # Save Saudi athletes separately
              saudi_df = combined_df[combined_df['country'].str.contains('KSA|Saudi', case=False, na=False)]
              if len(saudi_df) > 0:
                  athletes_dir = Path('data/athletes')
                  athletes_dir.mkdir(parents=True, exist_ok=True)
                  saudi_df.to_csv(athletes_dir / 'saudi_athletes.csv', index=False)
                  print(f"\n[KSA] Saved {len(saudi_df)} Saudi athletes to data/athletes/saudi_athletes.csv")

              print(f"\n{'='*60}")
              print(f"SCRAPING COMPLETE!")
              print(f"{'='*60}")
              print(f"Categories scraped: {categories_scraped}/16")
              print(f"Total athletes: {len(combined_df)}")
              print(f"Unique countries: {combined_df['country'].nunique()}")
              print(f"Saudi athletes: {len(saudi_df)}")
              print(f"Files saved to: data/rankings/")
          else:
              print("\n[ERROR] No data collected!")
              exit(1)
          EOF

          # Check results
          if [ -f "data/rankings/world_rankings_latest.csv" ]; then
            ATHLETE_COUNT=$(wc -l < "data/rankings/world_rankings_latest.csv")
            CATEGORY_COUNT=$(ls data/rankings/*_rankings_*.csv 2>/dev/null | wc -l)
            echo "athletes_collected=$((ATHLETE_COUNT - 1))" >> $GITHUB_OUTPUT
            echo "categories_scraped=$CATEGORY_COUNT" >> $GITHUB_OUTPUT
            echo "scrape_status=success" >> $GITHUB_OUTPUT
            echo "Collected $((ATHLETE_COUNT - 1)) athletes from $CATEGORY_COUNT categories"
          else
            echo "athletes_collected=0" >> $GITHUB_OUTPUT
            echo "categories_scraped=0" >> $GITHUB_OUTPUT
            echo "scrape_status=failed" >> $GITHUB_OUTPUT
            echo "No data collected"
          fi

      - name: Upload to Azure Blob Storage
        if: steps.scrape.outputs.scrape_status == 'success'
        env:
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
        run: |
          if [ -z "$AZURE_STORAGE_CONNECTION_STRING" ]; then
            echo "No Azure connection string configured - skipping upload"
            exit 0
          fi

          pip install azure-storage-blob

          python << 'EOF'
          import os
          from pathlib import Path
          from datetime import datetime
          from azure.storage.blob import BlobServiceClient

          connection_string = os.environ.get('AZURE_STORAGE_CONNECTION_STRING')
          if not connection_string:
              print("No Azure connection string")
              exit(0)

          blob_service = BlobServiceClient.from_connection_string(connection_string)
          container = blob_service.get_container_client('taekwondo-data')

          # Ensure container exists
          try:
              container.create_container()
          except:
              pass  # Already exists

          # Upload files
          for csv_file in Path('data').glob('**/*.csv'):
              blob_name = f"{csv_file.parent.name}/{csv_file.name}"
              with open(csv_file, 'rb') as data:
                  container.upload_blob(name=blob_name, data=data, overwrite=True)
                  print(f"Uploaded: {blob_name}")

          print("Azure upload complete")
          EOF

      - name: Commit data to repo
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"

          # Add scraped data files (CSV only)
          git add data/rankings/*.csv data/athletes/*.csv || true

          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Auto-update: Rankings data $(date +'%Y-%m-%d')"
            git push
            echo "Data committed and pushed to repo"
          fi

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rankings-data-${{ github.run_number }}
          path: data/
          if-no-files-found: ignore

      - name: Job Summary
        run: |
          echo "## World Taekwondo Rankings Scrape Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Status | ${{ steps.scrape.outputs.scrape_status || 'unknown' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Categories | ${{ steps.scrape.outputs.categories_scraped || '0' }}/16 |" >> $GITHUB_STEP_SUMMARY
          echo "| Total Athletes | ${{ steps.scrape.outputs.athletes_collected || '0' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Timestamp | $(date) |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "data/rankings/world_rankings_latest.csv" ]; then
            # Count unique countries
            COUNTRIES=$(tail -n +2 data/rankings/world_rankings_latest.csv | cut -d',' -f4 | sort -u | wc -l)
            echo "- **Unique Countries**: $COUNTRIES" >> $GITHUB_STEP_SUMMARY

            # Check for Saudi athletes
            SAUDI_COUNT=$(grep -i "KSA\|Saudi" data/rankings/world_rankings_latest.csv | wc -l)
            if [ "$SAUDI_COUNT" -gt 0 ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### Saudi Athletes Found" >> $GITHUB_STEP_SUMMARY
              grep -i "KSA\|Saudi" data/rankings/world_rankings_latest.csv >> $GITHUB_STEP_SUMMARY
            fi

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Sample Data (First 10 rows)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            head -11 data/rankings/world_rankings_latest.csv >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Files Generated" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            ls -la data/rankings/*.csv >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
