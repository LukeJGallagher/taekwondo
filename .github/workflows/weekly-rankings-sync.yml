# Weekly Rankings Sync - uploads scraped data to Azure
# This workflow is triggered after the main scraper runs

name: Weekly Taekwondo Rankings Sync

on:
  workflow_dispatch:

  # Run after the scraper workflow completes
  workflow_run:
    workflows: ["Weekly Rankings Scraper"]
    types:
      - completed

jobs:
  sync-to-azure:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install azure-storage-blob pandas

      - name: Download scraper artifacts
        if: github.event_name == 'workflow_run'
        uses: actions/download-artifact@v4
        with:
          name: rankings-data-${{ github.event.workflow_run.run_number }}
          path: data/

      - name: Sync to Azure Blob Storage
        env:
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
        run: |
          if [ -z "$AZURE_STORAGE_CONNECTION_STRING" ]; then
            echo "No Azure connection string - skipping sync"
            exit 0
          fi

          python << 'EOF'
          import os
          from pathlib import Path
          from datetime import datetime
          from azure.storage.blob import BlobServiceClient

          connection_string = os.environ.get('AZURE_STORAGE_CONNECTION_STRING')
          if not connection_string:
              print("No Azure connection string")
              exit(0)

          data_dir = Path('data')
          if not data_dir.exists():
              print("No data directory found")
              exit(0)

          csv_files = list(data_dir.glob('**/*.csv'))
          if not csv_files:
              print("No CSV files to upload")
              exit(0)

          print(f"Found {len(csv_files)} CSV files to upload")

          blob_service = BlobServiceClient.from_connection_string(connection_string)
          container = blob_service.get_container_client('taekwondo-data')

          # Ensure container exists
          try:
              container.create_container()
              print("Created container: taekwondo-data")
          except Exception as e:
              if 'ContainerAlreadyExists' not in str(e):
                  print(f"Container check: {e}")

          # Upload files
          for csv_file in csv_files:
              blob_name = f"{csv_file.parent.name}/{csv_file.name}"
              try:
                  with open(csv_file, 'rb') as data:
                      container.upload_blob(name=blob_name, data=data, overwrite=True)
                  print(f"Uploaded: {blob_name}")
              except Exception as e:
                  print(f"Failed to upload {blob_name}: {e}")

          print("\nAzure sync complete!")
          EOF

      - name: Summary
        run: |
          echo "## Rankings Sync Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Time**: $(date)" >> $GITHUB_STEP_SUMMARY

          if [ -d "data" ]; then
            FILE_COUNT=$(find data -name "*.csv" | wc -l)
            echo "- **Files synced**: $FILE_COUNT" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Files synced**: 0" >> $GITHUB_STEP_SUMMARY
          fi
