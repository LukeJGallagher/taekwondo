# Scrape Athlete Profiles from SimplyCompete
# Gets detailed win/loss records for top 20 + Asian Games rivals

name: Athlete Profile Scraper

on:
  workflow_dispatch:

  # Run after rankings scraper on Mondays
  workflow_run:
    workflows: ["Weekly Rankings Scraper"]
    types: [completed]

env:
  PYTHON_VERSION: '3.11'

jobs:
  scrape-profiles:
    name: Scrape Top 20 + Asian Rivals
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas selenium webdriver-manager requests beautifulsoup4

      - name: Create data directories
        run: |
          mkdir -p data/athletes
          mkdir -p data/profiles

      - name: Scrape Athlete Profiles - Top 20 + Asian Rivals
        id: scrape
        run: |
          python << 'EOF'
          import time
          import re
          import json
          import pandas as pd
          from pathlib import Path
          from datetime import datetime
          from selenium import webdriver
          from selenium.webdriver.common.by import By
          from selenium.webdriver.chrome.options import Options
          from selenium.webdriver.chrome.service import Service
          from selenium.webdriver.support.ui import WebDriverWait
          from selenium.webdriver.support import expected_conditions as EC
          from webdriver_manager.chrome import ChromeDriverManager
          from io import StringIO

          print("="*60)
          print("ATHLETE PROFILE SCRAPER - TOP 20 + ASIAN RIVALS")
          print("="*60)

          # Asian Games rival countries to track
          ASIAN_RIVALS = [
              'KOR', 'Korea', 'Republic of Korea',
              'IRI', 'Iran', 'Islamic Republic',
              'CHN', 'China', "People's Republic",
              'JPN', 'Japan',
              'UZB', 'Uzbekistan',
              'KAZ', 'Kazakhstan',
              'THA', 'Thailand',
              'VIE', 'Vietnam', 'Viet Nam',
              'IND', 'India',
              'TPE', 'Chinese Taipei', 'Taiwan',
              'JOR', 'Jordan',
              'KSA', 'Saudi', 'Arabia',
              'UAE', 'United Arab Emirates',
              'KUW', 'Kuwait',
              'QAT', 'Qatar',
              'BRN', 'Bahrain',
              'MAS', 'Malaysia',
              'INA', 'Indonesia',
              'PHI', 'Philippines',
              'MGL', 'Mongolia',
          ]

          # All 16 weight categories
          WEIGHT_CATEGORIES = {
              'M-54kg': ('M', 'M_54'),
              'M-58kg': ('M', 'M_58'),
              'M-63kg': ('M', 'M_63'),
              'M-68kg': ('M', 'M_68'),
              'M-74kg': ('M', 'M_74'),
              'M-80kg': ('M', 'M_80'),
              'M-87kg': ('M', 'M_87'),
              'M+87kg': ('M', 'M_87P'),
              'F-46kg': ('F', 'F_46'),
              'F-49kg': ('F', 'F_49'),
              'F-53kg': ('F', 'F_53'),
              'F-57kg': ('F', 'F_57'),
              'F-62kg': ('F', 'F_62'),
              'F-67kg': ('F', 'F_67'),
              'F-73kg': ('F', 'F_73'),
              'F+73kg': ('F', 'F_73P'),
          }

          # Setup Chrome
          chrome_options = Options()
          chrome_options.add_argument('--headless=new')
          chrome_options.add_argument('--no-sandbox')
          chrome_options.add_argument('--disable-dev-shm-usage')
          chrome_options.add_argument('--disable-gpu')
          chrome_options.add_argument('--window-size=1920,1080')
          chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

          service = Service(ChromeDriverManager().install())
          driver = webdriver.Chrome(service=service, options=chrome_options)
          driver.set_page_load_timeout(30)
          print("[OK] Chrome initialized")

          output_dir = Path('data/profiles')
          output_dir.mkdir(parents=True, exist_ok=True)

          all_rankings = []
          timestamp = datetime.now().strftime('%Y%m%d')
          base_url = "https://worldtaekwondo.simplycompete.com/widget/rankings?country=&lang=EN&cat="

          def is_asian_rival(country_text):
              """Check if athlete is from Asian Games rival country"""
              if not country_text:
                  return False
              for rival in ASIAN_RIVALS:
                  if rival.lower() in country_text.lower():
                      return True
              return False

          # STEP 1: Scrape rankings for all categories with extended data
          print("\n" + "="*60)
          print("STEP 1: SCRAPING RANKINGS FOR ALL 16 CATEGORIES")
          print("="*60)

          for cat_name, (gender, cat_code) in WEIGHT_CATEGORIES.items():
              print(f"\n[{cat_name}] Scraping...")
              url = f"{base_url}{cat_code}"

              try:
                  driver.get(url)
                  time.sleep(3)

                  WebDriverWait(driver, 15).until(
                      EC.presence_of_element_located((By.TAG_NAME, 'table'))
                  )

                  tables = driver.find_elements(By.TAG_NAME, 'table')
                  best_table = None
                  max_rows = 0

                  for table in tables:
                      rows = table.find_elements(By.TAG_NAME, 'tr')
                      if len(rows) > max_rows:
                          max_rows = len(rows)
                          best_table = table

                  if best_table and max_rows >= 2:
                      html = best_table.get_attribute('outerHTML')
                      dfs = pd.read_html(StringIO(html))

                      if dfs and len(dfs[0]) > 0:
                          df = dfs[0]

                          # Standardize columns
                          if len(df.columns) >= 5:
                              df.columns = ['rank', 'rank_change', 'athlete_name', 'country', 'points'] + list(df.columns[5:])
                              df = df[['rank', 'rank_change', 'athlete_name', 'country', 'points']].copy()

                          df['gender'] = gender
                          df['weight_category'] = cat_name
                          df['is_top20'] = df['rank'] <= 20
                          df['is_asian_rival'] = df['country'].apply(is_asian_rival)
                          df['scraped_at'] = datetime.now().isoformat()

                          all_rankings.append(df)

                          top20_count = (df['rank'] <= 20).sum()
                          asian_count = df['is_asian_rival'].sum()
                          print(f"  [OK] {len(df)} athletes | Top 20: {top20_count} | Asian rivals: {asian_count}")

              except Exception as e:
                  print(f"  [ERROR] {cat_name}: {e}")

              time.sleep(2)

          driver.quit()
          print("\n[OK] Browser closed")

          # Combine and analyze
          if all_rankings:
              combined_df = pd.concat(all_rankings, ignore_index=True)

              # Filter for dashboard: Top 20 globally OR Top 10 Asian rivals per category
              top20_df = combined_df[combined_df['is_top20']].copy()
              asian_df = combined_df[combined_df['is_asian_rival']].copy()

              # Get top 10 Asian rivals per category
              asian_top10 = asian_df.groupby('weight_category').head(10)

              # Combine unique athletes
              priority_athletes = pd.concat([top20_df, asian_top10]).drop_duplicates(
                  subset=['athlete_name', 'weight_category']
              )

              print(f"\n{'='*60}")
              print("SCRAPING SUMMARY")
              print(f"{'='*60}")
              print(f"Total athletes scraped: {len(combined_df)}")
              print(f"Categories: {combined_df['weight_category'].nunique()}/16")
              print(f"Countries: {combined_df['country'].nunique()}")
              print(f"Top 20 athletes: {len(top20_df)}")
              print(f"Asian rival athletes: {len(asian_df)}")
              print(f"Priority athletes (Top 20 + Asian Top 10): {len(priority_athletes)}")

              # Saudi athletes
              saudi_df = combined_df[combined_df['country'].str.contains('KSA|Saudi', case=False, na=False)]
              if len(saudi_df) > 0:
                  print(f"\n[KSA] Saudi Athletes Found: {len(saudi_df)}")
                  for _, row in saudi_df.iterrows():
                      print(f"  - {row['athlete_name']} | {row['weight_category']} | Rank #{row['rank']}")

              # Save files
              combined_df.to_csv(output_dir / f'all_rankings_{timestamp}.csv', index=False)
              combined_df.to_csv(output_dir / 'all_rankings_latest.csv', index=False)

              priority_athletes.to_csv(output_dir / f'priority_athletes_{timestamp}.csv', index=False)
              priority_athletes.to_csv(output_dir / 'priority_athletes_latest.csv', index=False)

              top20_df.to_csv(output_dir / 'top20_global.csv', index=False)
              asian_df.to_csv(output_dir / 'asian_rivals.csv', index=False)

              if len(saudi_df) > 0:
                  saudi_df.to_csv(output_dir / 'saudi_athletes.csv', index=False)

              # Summary JSON
              summary = {
                  'scraped_at': datetime.now().isoformat(),
                  'total_athletes': len(combined_df),
                  'categories': combined_df['weight_category'].nunique(),
                  'countries': combined_df['country'].nunique(),
                  'top20_count': len(top20_df),
                  'asian_rivals_count': len(asian_df),
                  'priority_count': len(priority_athletes),
                  'saudi_count': len(saudi_df),
                  'by_category': combined_df.groupby('weight_category').size().to_dict(),
                  'asian_countries': asian_df['country'].value_counts().head(15).to_dict()
              }

              with open(output_dir / 'scrape_summary.json', 'w') as f:
                  json.dump(summary, f, indent=2)

              print(f"\nFiles saved to: {output_dir}/")

          else:
              print("\n[ERROR] No data collected!")
              exit(1)
          EOF

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: athlete-profiles-${{ github.run_number }}
          path: data/profiles/
          if-no-files-found: ignore

      - name: Upload to Azure
        if: always()
        env:
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
        run: |
          if [ -z "$AZURE_STORAGE_CONNECTION_STRING" ]; then
            echo "No Azure connection - skipping"
            exit 0
          fi

          pip install azure-storage-blob

          python << 'EOF'
          import os
          from pathlib import Path
          from azure.storage.blob import BlobServiceClient

          conn = os.environ.get('AZURE_STORAGE_CONNECTION_STRING')
          if not conn:
              exit(0)

          blob_service = BlobServiceClient.from_connection_string(conn)
          container = blob_service.get_container_client('taekwondo-data')

          try:
              container.create_container()
          except:
              pass

          for f in Path('data/profiles').glob('*.csv'):
              blob_name = f'profiles/{f.name}'
              with open(f, 'rb') as data:
                  container.upload_blob(name=blob_name, data=data, overwrite=True)
                  print(f"Uploaded: {blob_name}")

          for f in Path('data/profiles').glob('*.json'):
              blob_name = f'profiles/{f.name}'
              with open(f, 'rb') as data:
                  container.upload_blob(name=blob_name, data=data, overwrite=True)
                  print(f"Uploaded: {blob_name}")
          EOF

      - name: Job Summary
        run: |
          echo "## Athlete Profile Scrape Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "data/profiles/scrape_summary.json" ]; then
            echo "### Statistics" >> $GITHUB_STEP_SUMMARY
            echo '```json' >> $GITHUB_STEP_SUMMARY
            cat data/profiles/scrape_summary.json >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f "data/profiles/all_rankings_latest.csv" ]; then
            TOTAL=$(wc -l < "data/profiles/all_rankings_latest.csv")
            echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Total Athletes | $((TOTAL - 1)) |" >> $GITHUB_STEP_SUMMARY

            if [ -f "data/profiles/top20_global.csv" ]; then
              TOP20=$(wc -l < "data/profiles/top20_global.csv")
              echo "| Top 20 Global | $((TOP20 - 1)) |" >> $GITHUB_STEP_SUMMARY
            fi

            if [ -f "data/profiles/asian_rivals.csv" ]; then
              ASIAN=$(wc -l < "data/profiles/asian_rivals.csv")
              echo "| Asian Rivals | $((ASIAN - 1)) |" >> $GITHUB_STEP_SUMMARY
            fi

            if [ -f "data/profiles/saudi_athletes.csv" ]; then
              SAUDI=$(wc -l < "data/profiles/saudi_athletes.csv")
              echo "| Saudi Athletes | $((SAUDI - 1)) |" >> $GITHUB_STEP_SUMMARY
            fi

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Top 20 Sample" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            head -11 data/profiles/top20_global.csv >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Asian Rivals Sample" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            head -11 data/profiles/asian_rivals.csv >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "No data collected" >> $GITHUB_STEP_SUMMARY
          fi
